#!/usr/bin/env python3
"""
éŸ³é¢‘-æ–‡æœ¬å¯¹é½æ•°æ®å‡†å¤‡è„šæœ¬ in Macbook M2

ä½¿ç”¨ stable-ts (MLX) ç”ŸæˆéŸ³é¢‘å’Œæ–‡æœ¬çš„å¯¹é½æ•°æ®
æ”¯æŒæ‰¹é‡å¤„ç† 00001.mp3 ~ 99999.mp3 æ ¼å¼çš„æ–‡ä»¶

ç”¨æ³•:
    1. ç›´æ¥åœ¨ä¸‹æ–¹ã€ç”¨æˆ·é…ç½®åŒºåŸŸã€‘è®¾ç½®å‚æ•°
    2. è¿è¡Œ: python prepare_alignment.py
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any

try:
    import stable_whisper
    from tqdm import tqdm
except ImportError:
    print("é”™è¯¯: è¯·å…ˆå®‰è£…ä¾èµ–")
    print("è¿è¡Œ: pip install -r requirements.txt")
    sys.exit(1)


# ============================================================
# ç”¨æˆ·é…ç½®åŒºåŸŸ - åœ¨è¿™é‡Œè®¾ç½®ä½ çš„å‚æ•°
# ============================================================

# è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å« 00001.mp3, 00001.txt ç­‰æ–‡ä»¶ï¼‰
INPUT_FOLDER = "/Users/max/dev/Audiobook_web_APP/audiobook_files"

# è¾“å‡ºæ–‡ä»¶è·¯å¾„
OUTPUT_ALIGNMENT_JSON = "alignment.json"
OUTPUT_MERGED_AUDIO = "merged_book.mp3"
OUTPUT_MERGED_TXT = "merged_book.txt"  # åˆå¹¶åçš„æ–‡æœ¬æ–‡ä»¶

# Whisper æ¨¡å‹å¤§å°: "tiny", "base", "small", "medium", "large"
MODEL_SIZE = "medium"

# è¯­è¨€ä»£ç : "zh" (ä¸­æ–‡), "en" (è‹±æ–‡), "ja" (æ—¥æ–‡) ç­‰
LANGUAGE = "zh"

# æ˜¯å¦åˆå¹¶éŸ³é¢‘æ–‡ä»¶
MERGE_AUDIO = True

# éŸ³é¢‘æ–‡ä»¶ä¹‹é—´çš„é—´éš”æ—¶é•¿ï¼ˆç§’ï¼‰
GAP_SECONDS = 1.0

# æ˜¯å¦åˆå¹¶æ–‡æœ¬æ–‡ä»¶
MERGE_TXT = True

# ============================================================
# ä»¥ä¸‹æ˜¯è„šæœ¬ä»£ç ï¼Œæ— éœ€ä¿®æ”¹
# ============================================================




def load_audio_files(input_dir: Path) -> List[tuple]:
    """
    åŠ è½½éŸ³é¢‘æ–‡ä»¶å’Œå¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶
    
    è¿”å›: [(audio_path, text_path, index), ...]
    """
    files = []
    
    for audio_file in sorted(input_dir.glob("*.mp3")):
        # è·å–æ–‡ä»¶ç¼–å· (ä¾‹å¦‚ 00001.mp3 -> 00001)
        stem = audio_file.stem
        
        # æŸ¥æ‰¾å¯¹åº”çš„æ–‡æœ¬æ–‡ä»¶
        txt_file = input_dir / f"{stem}.txt"
        
        if txt_file.exists():
            files.append((audio_file, txt_file, stem))
        else:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ° {stem}.txtï¼Œè·³è¿‡ {audio_file.name}")
    
    return files


def transcribe_with_alignment(
    audio_path: Path,
    text_path: Path,
    model_name: str = "medium",
    language: str = "zh"
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ stable-ts å¯¹å•ä¸ªéŸ³é¢‘æ–‡ä»¶è¿›è¡Œå¯¹é½
    
    å‚æ•°:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        text_path: æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        model_name: Whisper æ¨¡å‹åç§°
        language: è¯­è¨€ä»£ç 
    
    è¿”å›:
        å¯¹é½ç»“æœå­—å…¸ï¼ˆåŒ…å«è¯çº§åˆ«æ—¶é—´æˆ³ï¼‰
    """
    # è¯»å–å‚è€ƒæ–‡æœ¬
    with open(text_path, 'r', encoding='utf-8') as f:
        reference_text = f.read().strip()
    
    # å°è¯•ä½¿ç”¨ MLX åŠ é€Ÿ
    use_mlx = False
    try:
        import mlx_whisper
        print(f"  åŠ è½½æ¨¡å‹: {model_name} (MLX GPU åŠ é€Ÿ)")
        # ä½¿ç”¨ mlx-whisper è¿›è¡Œè½¬å½•ï¼ˆGPU åŠ é€Ÿï¼‰
        result_mlx = mlx_whisper.transcribe(
            str(audio_path),
            path_or_hf_repo=f"mlx-community/whisper-{model_name}-mlx",
            verbose=False,
            language=language,
            word_timestamps=True  # å¯ç”¨è¯çº§åˆ«æ—¶é—´æˆ³
        )
        use_mlx = True
        print("  âœ“ MLX GPU åŠ é€Ÿå·²å¯ç”¨")
        
        # æå–è¯çº§åˆ«å¯¹é½æ•°æ®
        segments = []
        segment_id = 0
        
        for segment in result_mlx["segments"]:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¯çº§åˆ«æ•°æ®
            if "words" in segment and segment["words"]:
                # ä½¿ç”¨è¯çº§åˆ«æ—¶é—´æˆ³
                for word_data in segment["words"]:
                    word_text = word_data.get("word", "").strip()
                    if not word_text:
                        continue
                    seg_data = {
                        "id": segment_id,
                        "start": round(word_data["start"], 2),
                        "end": round(word_data["end"], 2),
                        "text": word_text
                    }
                    segments.append(seg_data)
                    segment_id += 1
            else:
                # å›é€€åˆ°å¥å­çº§åˆ«
                seg_data = {
                    "id": segment_id,
                    "start": round(segment["start"], 2),
                    "end": round(segment["end"], 2),
                    "text": segment["text"].strip()
                }
                segments.append(seg_data)
                segment_id += 1
        
        return {
            "segments": segments,
            "language": language,
            "duration": round(max([s["end"] for s in segments]) if segments else 0, 2)
        }
        
    except Exception as e:
        print(f"  âš  MLX GPU åŠ é€Ÿä¸å¯ç”¨: {e}")
        print("\n" + "="*60)
        print("è­¦å‘Š: æ— æ³•ä½¿ç”¨ GPU åŠ é€Ÿï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
        print("="*60)
        
        # è¦æ±‚ç”¨æˆ·ç¡®è®¤
        user_input = input("\næ˜¯å¦ç»§ç»­ä½¿ç”¨ CPU æ¨¡å¼ï¼Ÿè¾“å…¥ YES ç»§ç»­ï¼Œå…¶ä»–ä»»ä½•è¾“å…¥å°†é€€å‡º: ").strip()
        
        if user_input != "YES":
            print("\nç”¨æˆ·å–æ¶ˆï¼Œç¨‹åºé€€å‡ºã€‚")
            sys.exit(0)
        
        print("\nç”¨æˆ·ç¡®è®¤ï¼Œç»§ç»­ä½¿ç”¨ CPU æ¨¡å¼...\n")
        use_mlx = False
    
    # å¦‚æœ MLX å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡† stable-tsï¼ˆCPUï¼‰
    if not use_mlx:
        print(f"  åŠ è½½æ¨¡å‹: {model_name} (CPU æ¨¡å¼)")
        model = stable_whisper.load_model(model_name)
        
        # è½¬å½•å¹¶å¯¹é½ï¼ˆåªéœ€è¦å¥å­çº§åˆ«æ—¶é—´æˆ³ï¼‰
        print(f"  å¤„ç†éŸ³é¢‘: {audio_path.name}")
        result = model.transcribe(
            str(audio_path),
            language=language,
            word_timestamps=True,  # å¯ç”¨è¯çº§åˆ«æ—¶é—´æˆ³
            initial_prompt=reference_text[:100],  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºæç¤º
            vad=False  # ç¦ç”¨ VAD ä»¥åŠ å¿«é€Ÿåº¦
        )
        
        # æå–è¯çº§åˆ«å¯¹é½æ•°æ®
        segments = []
        segment_id = 0
        
        for segment in result.segments:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¯çº§åˆ«æ•°æ®
            if hasattr(segment, 'words') and segment.words:
                for word in segment.words:
                    word_text = word.word.strip() if hasattr(word, 'word') else str(word).strip()
                    if not word_text:
                        continue
                    seg_data = {
                        "id": segment_id,
                        "start": round(word.start, 2),
                        "end": round(word.end, 2),
                        "text": word_text
                    }
                    segments.append(seg_data)
                    segment_id += 1
            else:
                # å›é€€åˆ°å¥å­çº§åˆ«
                seg_data = {
                    "id": segment_id,
                    "start": round(segment.start, 2),
                    "end": round(segment.end, 2),
                    "text": segment.text.strip()
                }
                segments.append(seg_data)
                segment_id += 1
        
        return {
            "segments": segments,
            "language": language,
            "duration": round(result.duration, 2) if hasattr(result, 'duration') else 0
        }




def merge_audio_files(audio_files: List[Path], output_path: Path, gap_seconds: float = 1.0):
    """
    åˆå¹¶å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ï¼Œå¹¶åœ¨æ¯ä¸ªæ–‡ä»¶ä¹‹é—´æ’å…¥é™éŸ³é—´éš”
    
    å‚æ•°:
        audio_files: éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        gap_seconds: é—´éš”æ—¶é•¿ï¼ˆç§’ï¼‰
    """
    try:
        from pydub import AudioSegment
    except ImportError:
        print("è­¦å‘Š: pydub æœªå®‰è£…ï¼Œè·³è¿‡éŸ³é¢‘åˆå¹¶")
        return
    
    print("\nåˆå¹¶éŸ³é¢‘æ–‡ä»¶...")
    combined = AudioSegment.empty()
    silence = AudioSegment.silent(duration=int(gap_seconds * 1000))
    
    for i, audio_file in enumerate(tqdm(audio_files, desc="åˆå¹¶è¿›åº¦")):
        audio = AudioSegment.from_mp3(audio_file)
        combined += audio
        
        # æœ€åä¸€ä¸ªæ–‡ä»¶åä¸æ·»åŠ é™éŸ³
        if i < len(audio_files) - 1:
            combined += silence
    
    combined.export(output_path, format="mp3")
    print(f"åˆå¹¶å®Œæˆ: {output_path}")


def adjust_timestamps_for_merged(
    alignments: List[Dict[str, Any]],
    gap_seconds: float = 1.0
) -> Dict[str, Any]:
    """
    è°ƒæ•´æ—¶é—´æˆ³ä»¥é€‚åº”åˆå¹¶åçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆè¯çº§åˆ«ï¼‰
    
    å‚æ•°:
        alignments: å„ä¸ªéŸ³é¢‘æ–‡ä»¶çš„å¯¹é½æ•°æ®åˆ—è¡¨
        gap_seconds: é—´éš”æ—¶é•¿ï¼ˆç§’ï¼‰- éŸ³é¢‘åˆå¹¶æ—¶åœ¨æ¯ä¸ªæ–‡ä»¶ä¹‹é—´æ’å…¥çš„é™éŸ³æ—¶é•¿
    
    è¿”å›:
        åˆå¹¶åçš„å¯¹é½æ•°æ®
    """
    merged_segments = []
    current_offset = 0.0
    segment_id = 0
    
    for i, alignment in enumerate(alignments):
        for segment in alignment["segments"]:
            adjusted_segment = {
                "id": segment_id,
                "start": round(segment["start"] + current_offset, 2),
                "end": round(segment["end"] + current_offset, 2),
                "text": segment["text"]
            }
            
            merged_segments.append(adjusted_segment)
            segment_id += 1
        
        # æ›´æ–°åç§»é‡ï¼ˆå½“å‰éŸ³é¢‘æ—¶é•¿ + é—´éš”ï¼‰
        current_offset += alignment["duration"] + gap_seconds
    
    return {
        "segments": merged_segments,
        "language": alignments[0]["language"] if alignments else "zh",
        "duration": round(current_offset - gap_seconds, 2)  # å‡å»æœ€åä¸€ä¸ªé—´éš”
    }


def merge_txt_files(txt_files: List[Path], output_path: Path):
    """
    åˆå¹¶å¤šä¸ªæ–‡æœ¬æ–‡ä»¶ä¸ºä¸€ä¸ªæ–‡ä»¶
    
    å‚æ•°:
        txt_files: æ–‡æœ¬æ–‡ä»¶åˆ—è¡¨ï¼ˆå·²æŒ‰é¡ºåºæ’åˆ—ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    print("\nåˆå¹¶æ–‡æœ¬æ–‡ä»¶...")
    merged_content = []
    
    for txt_file in tqdm(txt_files, desc="åˆå¹¶è¿›åº¦"):
        with open(txt_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if content:
                merged_content.append(content)
    
    # ç”¨æ¢è¡Œç¬¦åˆ†éš”å„ä¸ªæ–‡ä»¶å†…å®¹
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(merged_content))
    
    print(f"åˆå¹¶å®Œæˆ: {output_path}")
    print(f"  - æ–‡ä»¶æ•°: {len(txt_files)}")
    print(f"  - æ€»å­—ç¬¦æ•°: {sum(len(c) for c in merged_content)}")


def main():
    parser = argparse.ArgumentParser(
        description="éŸ³é¢‘-æ–‡æœ¬å¯¹é½æ•°æ®å‡†å¤‡å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æç¤º: ä½ å¯ä»¥ç›´æ¥åœ¨è„šæœ¬é¡¶éƒ¨çš„ã€ç”¨æˆ·é…ç½®åŒºåŸŸã€‘è®¾ç½®å‚æ•°ï¼Œ
     ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–è¿™äº›è®¾ç½®ã€‚
        """
    )
    parser.add_argument(
        "--input-dir",
        type=Path,
        default=Path(INPUT_FOLDER),
        help=f"åŒ…å«éŸ³é¢‘å’Œæ–‡æœ¬æ–‡ä»¶çš„ç›®å½• (é»˜è®¤: {INPUT_FOLDER})"
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path(INPUT_FOLDER).joinpath(OUTPUT_ALIGNMENT_JSON),
        help=f"è¾“å‡ºçš„å¯¹é½æ•°æ®æ–‡ä»¶ (é»˜è®¤: {Path(INPUT_FOLDER).joinpath(OUTPUT_ALIGNMENT_JSON)})"
    )
    parser.add_argument(
        "--model",
        type=str,
        default=MODEL_SIZE,
        choices=["tiny", "base", "small", "medium", "large"],
        help=f"Whisper æ¨¡å‹å¤§å° (é»˜è®¤: {MODEL_SIZE})"
    )
    parser.add_argument(
        "--language",
        type=str,
        default=LANGUAGE,
        help=f"è¯­è¨€ä»£ç  (é»˜è®¤: {LANGUAGE})"
    )
    parser.add_argument(
        "--merge-audio",
        action="store_true",
        default=MERGE_AUDIO,
        help=f"æ˜¯å¦åˆå¹¶éŸ³é¢‘æ–‡ä»¶ (é»˜è®¤: {MERGE_AUDIO})"
    )
    parser.add_argument(
        "--merged-audio-output",
        type=Path,
        default=Path(INPUT_FOLDER).joinpath(OUTPUT_MERGED_AUDIO),
        help=f"åˆå¹¶åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {Path(INPUT_FOLDER).joinpath(OUTPUT_MERGED_AUDIO)})"
    )
    parser.add_argument(
        "--gap",
        type=float,
        default=GAP_SECONDS,
        help=f"éŸ³é¢‘æ–‡ä»¶ä¹‹é—´çš„é—´éš”æ—¶é•¿ï¼ˆç§’ï¼Œé»˜è®¤: {GAP_SECONDS}ï¼‰"
    )
    parser.add_argument(
        "--merge-txt",
        action="store_true",
        default=MERGE_TXT,
        help=f"æ˜¯å¦åˆå¹¶æ–‡æœ¬æ–‡ä»¶ (é»˜è®¤: {MERGE_TXT})"
    )
    parser.add_argument(
        "--merged-txt-output",
        type=Path,
        default=Path(INPUT_FOLDER).joinpath(OUTPUT_MERGED_TXT),
        help=f"åˆå¹¶åçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ (é»˜è®¤: {Path(INPUT_FOLDER).joinpath(OUTPUT_MERGED_TXT)})"
    )

    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not args.input_dir.exists():
        print(f"é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        sys.exit(1)
    
    # ============================================================
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
    # ============================================================
    alignment_exists = args.output.exists()
    merged_audio_exists = args.merged_audio_output.exists() if args.merge_audio else False
    merged_txt_exists = args.merged_txt_output.exists() if args.merge_txt else False
    
    if alignment_exists:
        print(f"âœ“ å¯¹é½æ–‡ä»¶å·²å­˜åœ¨: {args.output}")
    
    if merged_audio_exists:
        print(f"âœ“ åˆå¹¶éŸ³é¢‘å·²å­˜åœ¨: {args.merged_audio_output}")
    
    if merged_txt_exists:
        print(f"âœ“ åˆå¹¶æ–‡æœ¬å·²å­˜åœ¨: {args.merged_txt_output}")
    
    # å¦‚æœæ‰€æœ‰åŠŸèƒ½éƒ½å·²å®Œæˆï¼Œè¯¢é—®æ˜¯å¦è·³è¿‡
    all_exist = (
        alignment_exists and 
        (not args.merge_audio or merged_audio_exists) and
        (not args.merge_txt or merged_txt_exists)
    )
    if all_exist:
        print("\n" + "="*60)
        print("æ‰€æœ‰è¾“å‡ºæ–‡ä»¶éƒ½å·²å­˜åœ¨ï¼")
        print("="*60)
        user_input = input("\næ˜¯å¦é‡æ–°ç”Ÿæˆï¼Ÿè¾“å…¥ YES é‡æ–°ç”Ÿæˆï¼Œå…¶ä»–ä»»ä½•è¾“å…¥å°†é€€å‡º: ").strip()
        
        if user_input != "YES":
            print("\nç”¨æˆ·é€‰æ‹©è·³è¿‡ï¼Œç¨‹åºé€€å‡ºã€‚")
            sys.exit(0)
        
        print("\nç”¨æˆ·ç¡®è®¤ï¼Œå°†é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶...\n")
        alignment_exists = False
        merged_audio_exists = False
        merged_txt_exists = False
    
    # åŠ è½½éŸ³é¢‘æ–‡ä»¶
    print(f"\næ‰«æç›®å½•: {args.input_dir}")
    audio_files = load_audio_files(args.input_dir)
    
    if not audio_files:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•éŸ³é¢‘-æ–‡æœ¬é…å¯¹æ–‡ä»¶")
        sys.exit(1)
    
    print(f"æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘-æ–‡æœ¬é…å¯¹")
    
    # ============================================================
    # åŠŸèƒ½ 1: ç”Ÿæˆå¯¹é½æ•°æ®
    # ============================================================
    if alignment_exists:
        print("\nâ­ï¸  è·³è¿‡å¯¹é½æ•°æ®ç”Ÿæˆï¼ˆæ–‡ä»¶å·²å­˜åœ¨ï¼‰")
        # åŠ è½½ç°æœ‰å¯¹é½æ•°æ®ä»¥ä¾›åç»­ä½¿ç”¨
        with open(args.output, 'r', encoding='utf-8') as f:
            merged_alignment = json.load(f)
        audio_paths = [audio_path for audio_path, _, _ in audio_files]
    else:
        print("\nğŸ“ å¼€å§‹ç”Ÿæˆå¯¹é½æ•°æ®...")
        # å¤„ç†æ¯ä¸ªéŸ³é¢‘æ–‡ä»¶
        alignments = []
        audio_paths = []
        
        for audio_path, text_path, stem in tqdm(audio_files, desc="å¤„ç†è¿›åº¦"):
            print(f"\nå¤„ç† {stem}:")
            
            try:
                alignment = transcribe_with_alignment(
                    audio_path,
                    text_path,
                    model_name=args.model,
                    language=args.language
                )
                alignments.append(alignment)
                audio_paths.append(audio_path)
                
            except Exception as e:
                print(f"  é”™è¯¯: {e}")
                continue
        
        if not alignments:
            print("é”™è¯¯: æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ–‡ä»¶")
            sys.exit(1)
        
        # åˆå¹¶å¯¹é½æ•°æ®
        print("\nåˆå¹¶å¯¹é½æ•°æ®...")
        merged_alignment = adjust_timestamps_for_merged(alignments, args.gap)
        
        # ä¿å­˜å¯¹é½æ•°æ®
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(merged_alignment, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… å¯¹é½æ•°æ®å·²ä¿å­˜: {args.output}")
        print(f"  - æ€»æ®µè½æ•°: {len(merged_alignment['segments'])}")
        print(f"  - æ€»æ—¶é•¿: {merged_alignment['duration']:.2f} ç§’")
    
    # ============================================================
    # åŠŸèƒ½ 2: åˆå¹¶éŸ³é¢‘æ–‡ä»¶
    # ============================================================
    if args.merge_audio:
        if merged_audio_exists:
            print("\nâ­ï¸  è·³è¿‡éŸ³é¢‘åˆå¹¶ï¼ˆæ–‡ä»¶å·²å­˜åœ¨ï¼‰")
        else:
            print("\nğŸµ å¼€å§‹åˆå¹¶éŸ³é¢‘...")
            merge_audio_files(audio_paths, args.merged_audio_output, args.gap)
    
    # ============================================================
    # åŠŸèƒ½ 3: åˆå¹¶æ–‡æœ¬æ–‡ä»¶
    # ============================================================
    if args.merge_txt:
        if merged_txt_exists:
            print("\nâ­ï¸  è·³è¿‡æ–‡æœ¬åˆå¹¶ï¼ˆæ–‡ä»¶å·²å­˜åœ¨ï¼‰")
        else:
            print("\nğŸ“„ å¼€å§‹åˆå¹¶æ–‡æœ¬...")
            txt_paths = [txt_path for _, txt_path, _ in audio_files]
            merge_txt_files(txt_paths, args.merged_txt_output)
    
    print("\n" + "="*60)
    print("âœ… å®Œæˆï¼")
    print("="*60)



if __name__ == "__main__":
    main()
